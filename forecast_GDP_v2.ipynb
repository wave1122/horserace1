{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Match a date in the GDP dataframe with another date before that date in another dataframe ================================= #\n",
    "# ======================================================================================================================== #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "##### Set the current working directory\n",
    "path=\"e:\\\\Copy\\\\SCRIPTS\\\\Forecast_Covid_Recovery\\\\Code\\\\\"\n",
    "os.chdir(path)\n",
    "\n",
    "##### parse dates and times\n",
    "def date_parser(date): \n",
    "    dt = datetime.datetime.strptime(date, '%m/%d/%Y')\n",
    "    return dt.strftime('%Y-%m-%d') \n",
    "\n",
    "##### parse dates and times\n",
    "def date_parser2(date): \n",
    "    dt = datetime.datetime.strptime(date, '%Y:%m:%d')\n",
    "    return dt.strftime('%Y-%m-%d') \n",
    "\n",
    "##### Find the date which is 'n' days right after a date 'pivot' \n",
    "def closest_dates_after(date_list, pivot, n):\n",
    "    key=lambda x: abs(x - pivot) \n",
    "    dates = [i for i in date_list if i > pivot]\n",
    "    closest_dates = sorted(dates, key = key)[:n]\n",
    "    found_dates = [str( closest_date.date() ) for closest_date in closest_dates] # get a list of string values of dates\n",
    "    return found_dates[n-1] \n",
    "\n",
    "##### Find the date which is 'n' days before a date 'pivot' (if 'n' = 1, then the date found can be similar to 'pivot')\n",
    "def closest_dates_before(date_list, pivot, n):\n",
    "    key=lambda x: abs(x - pivot) \n",
    "    dates = [i for i in date_list if i <= pivot]\n",
    "    closest_dates = sorted(dates, key = key)[:n]\n",
    "    found_dates = [str( closest_date.date() ) for closest_date in closest_dates] # get a list of string values of dates\n",
    "    return found_dates[n-1]\n",
    "\n",
    "##### Combine two dataframes with dates of the second frame are 'n' periods on/before the dates of the first frame\n",
    "def combine_df_before_dates(df1, df2, n):\n",
    "    date_list1 = pd.to_datetime(df1['date'].tolist(), format='%Y-%m-%d')\n",
    "    date_list2 = pd.to_datetime(df2['date'].tolist(), format='%Y-%m-%d')\n",
    "    list_date1 = []\n",
    "    list_date2 = []\n",
    "    for date1 in date_list1:\n",
    "        closest_date = closest_dates_before(date_list2, date1, n)\n",
    "        list_date1.append( str( date1.date() ) )\n",
    "        list_date2.append(closest_date)\n",
    "    df1_new = df1.set_index('date').loc[list_date1, :].reset_index()\n",
    "    df2_new = df2.set_index('date').loc[list_date2, :].reset_index()\n",
    "    colnames2 = df2_new.columns.tolist()\n",
    "    df2_new.set_axis([f'{colnames2[0]}_lag_{n}', f'{colnames2[1]}_lag_{n}'], axis = 1, inplace = True)\n",
    "    combined_df = pd.concat([df1_new, df2_new], axis=1, join='inner')\n",
    "    return combined_df\n",
    "\n",
    "# import GDP data\n",
    "colnames_GDP = ['date', 'GDP', 'OUTBS', 'OUTNFB', 'GPDIC1', 'INDPRO', 'CMRMTSPLx', 'IPMAT', 'FPIx', 'IPFINAL', 'IPDMAT']\n",
    "fields = ['date', 'GDP', 'OUTBS', 'OUTNFB', 'GPDIC1', 'INDPRO', 'CMRMTSPLx', 'IPMAT', 'FPIx', 'IPFINAL', 'IPDMAT']\n",
    "US_df = pd.read_csv('../Data\\\\FRED\\\\quarterly_transformed_pdc_sis.csv', engine = 'python', encoding='utf-8', \\\n",
    "                                                                                                                                                            skipinitialspace=True, usecols = fields, sep = ',', parse_dates=True)\n",
    "US_df.set_axis(colnames_GDP, axis = 1, inplace = True)\n",
    "US_df['date'] = US_df['date'].astype(str).apply(date_parser) # parse dates\n",
    "US_df.dropna(inplace=True)\n",
    "print( US_df.tail(10) )\n",
    "\n",
    "# import WEI\n",
    "fields = ['date', 'WEI']\n",
    "colnames=[\"date\", \"WEI\"]\n",
    "WEI_df = pd.read_csv('../Data\\\\weekly-economic-index_data.csv', encoding='utf-8', skipinitialspace=True, usecols = fields, sep = ',', parse_dates = True)\n",
    "WEI_df.set_axis(colnames, axis = 1, inplace = True)\n",
    "WEI_df.dropna(inplace=True)\n",
    "WEI_df['date'] = WEI_df['date'].astype(str).apply(date_parser) # parse dates\n",
    "print( WEI_df.head(20) )\n",
    "\n",
    "N = 6 # set maximum number of days before each release date of GDP\n",
    "US1_df = US_df.loc[US_df['date'] >= WEI_df.loc[0, 'date'], :].copy()\n",
    "print( US1_df.head() )\n",
    "combined_df = reduce( lambda left, right: pd.merge(left, right, on = colnames_GDP), [combine_df_before_dates(US1_df, WEI_df, n) for n in np.arange(1, N)] )\n",
    "col_removed = combined_df.columns.str.contains('date_lag')\n",
    "combined_df = combined_df[combined_df.columns[col_removed == False] ].copy() # remove the selected columns\n",
    "combined_df.to_csv('../Data/combined_data_WEI.csv', index=False, header = True)\n",
    "\n",
    "# import ADS Index\n",
    "fields_ADS = ['date', 'ADS_Index']\n",
    "US_ADS_df = pd.read_csv('../Data\\\\ADS_Index_Most_Current_Vintage.csv', engine = 'python', encoding='utf-8', skipinitialspace=True, usecols = fields_ADS, sep = ',')\n",
    "US_ADS_df.date = pd.to_datetime( US_ADS_df.date.astype(str).apply(date_parser2) )# parse dates\n",
    "US_ADS_df.dropna(inplace=True)\n",
    "US_ADS_df.set_index('date', inplace=True)\n",
    "US_ADS_df_monthly = US_ADS_df.copy().resample('M').mean().reset_index() # convert a time series to the monthly frequencey by sampling then taking mean\n",
    "print( US_ADS_df_monthly.head(20) )\n",
    "\n",
    "N = 6 # set maximum number of days before each release date of GDP\n",
    "US1_df = US_df.loc[pd.to_datetime(US_df['date']) >= US_ADS_df_monthly.loc[N-2, 'date'], :].copy()\n",
    "print( US1_df.head() )\n",
    "combined_df = reduce( lambda left, right: pd.merge(left, right, on = colnames_GDP), [combine_df_before_dates(US1_df, US_ADS_df_monthly, n) for n in np.arange(1, N)] )\n",
    "col_removed = combined_df.columns.str.contains('date_lag')\n",
    "combined_df = combined_df[combined_df.columns[col_removed == False] ].copy() # remove the selected columns\n",
    "combined_df.to_csv('../Data/combined_data_ADSI.csv', index=False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================== Transform FRED-QD data =================================================== #\n",
    "# =========================================================================================================================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "##### Set the current working directory\n",
    "path=\"e:\\\\Copy\\\\SCRIPTS\\\\Forecast_Covid_Recovery\\\\Code\\\\\"\n",
    "os.chdir(path)\n",
    "\n",
    "##### parse dates and times\n",
    "def date_parser(date): \n",
    "    dt = datetime.datetime.strptime(date, '%m/%d/%Y')\n",
    "    return dt.strftime('%Y-%m-%d') \n",
    "\n",
    "##### Define data transformations\n",
    "def transform_2(x: pd.Series):\n",
    "    return x.diff(periods = 1)\n",
    "\n",
    "def transform_3(x: pd.Series):\n",
    "    x1 = transform_2(x)\n",
    "    return x1.diff(periods = 1)\n",
    "\n",
    "def transform_4(x: pd.Series):\n",
    "    return np.log(x)\n",
    "\n",
    "def transform_5(x: pd.Series):\n",
    "    return np.log( x / x.shift(1) )\n",
    "\n",
    "def transform_6(x: pd.Series):\n",
    "    x1 = transform_5(x)\n",
    "    return x1.diff(periods = 1)\n",
    "\n",
    "def transform_7(x: pd.Series):\n",
    "    x1 = x / x.shift(1) - 1.0\n",
    "    return x1.diff(periods = 1)\n",
    "\n",
    "US_df = pd.read_csv('../Data\\\\FRED\\\\quarterly1.csv', engine = 'python', encoding='utf-8', \\\n",
    "                                                                                                                                            skipinitialspace=True, sep = ',', parse_dates = ['date'], index_col = 'date')\n",
    "US_df.fillna(US_df.mean(axis = 0), inplace = True)\n",
    "\n",
    "# transform the data variables\n",
    "step = 1\n",
    "for i in list(np.arange(58, 64 + step, step) ) + list(np.arange(78, 79 + step, step) ) + list(np.arange(144, 151 + step, step) ) + list(np.arange(189, 193 + step, step) ) \\\n",
    "        + list(np.arange(197, 198 + step, step) ) + list(np.arange(201, 201 + step, step) ) + list(np.arange(220, 220 + step, step) ) + list(np.arange(223, 225 + step, step) ) \\\n",
    "        + list(np.arange(238, 238 + step, step) ) + list(np.arange(243, 243 + step, step) ) + list(np.arange(247, 247 + step, step) ):\n",
    "    US_df.iloc[:, i-1] = transform_2( US_df.iloc[:, i-1] )\n",
    "\n",
    "for i in list(np.arange(1, 10 + step, step) ) + list(np.arange(12, 12 + step, step) ) + list(np.arange(14, 32 + step, step) ) + list(np.arange(35, 57 + step, step) ) \\\n",
    "        + list(np.arange(65, 76 + step, step) ) + list(np.arange(81, 94 + step, step) ) + list(np.arange(128, 143 + step, step) ) + list(np.arange(158, 187 + step, step) ) \\\n",
    "        + list(np.arange(194, 196 + step, step) ) + list(np.arange(221, 222 + step, step) ) + list(np.arange(227, 232 + step, step) ) + list(np.arange(234, 234 + step, step) ) \\\n",
    "        + list(np.arange(236, 237 + step, step) ) + list(np.arange(239, 239 + step, step) ) + list(np.arange(241, 242 + step, step) ) + list(np.arange(244, 246 + step, step) ) \\\n",
    "        + list(np.arange(248, 248 + step, step) ):\n",
    "    US_df.iloc[:, i-1] = transform_5( US_df.iloc[:, i-1] )\n",
    "\n",
    "for i in list(np.arange(95, 127 + step, step) ) + list(np.arange(199, 199 + step, step) ) + list(np.arange(205, 219 + step, step) ) + list(np.arange(233, 233 + step, step) ):\n",
    "    US_df.iloc[:, i-1] = transform_6( US_df.iloc[:, i-1] )\n",
    "\n",
    "for i in list(np.arange(200, 200 + step, step) ):\n",
    "    US_df.iloc[:, i-1] = transform_7( US_df.iloc[:, i-1] )\n",
    "\n",
    "US_df.to_csv('../Data/FRED/quarterly_transformed.csv', index=True, header = True)\n",
    "# US_df.plot(subplots=False, color='blue', figsize=(10, 8), grid=True, rot=10, sharex=True, sharey=False)\n",
    "# US_df.dropna(inplace=True)\n",
    " # drop variables with many missing observations\n",
    "US_df_PCA = US_df.drop(['OUTMS', 'TCU', 'LNS13023621',\t'LNS13023557', 'LNS13023705', 'LNS13023569', 'HOAMS', \\\n",
    "    'ACOGNOx', 'ANDENOx', 'INVCQRMTSPL', 'COMPRMS', 'OPHMFG', 'ULCMFG', 'IMFSLx', 'DRIWCIL', 'USSTHPI', \\\n",
    "    'SPCS10RSA', 'SPCS20RSA', 'TWEXAFEGSMTHx',\t'EXUSEU', 'USEPUINDXM', 'MORTG10YRx', 'MORTGAGE30US'], axis = 1)\n",
    "US_df_PCA.dropna(inplace=True)\n",
    "US_df_PCA.to_csv('../Data/FRED/quarterly_transformed_pca.csv', index=True, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BaChu\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'e:\\\\Copy\\\\SCRIPTS\\\\Forecast Covid Recovery\\\\Code\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Copy\\SCRIPTS\\Forecast_Covid_Recovery\\Code\\forecast_GDP_v2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Copy/SCRIPTS/Forecast_Covid_Recovery/Code/forecast_GDP_v2.ipynb#ch0000004?line=58'>59</a>\u001b[0m \u001b[39m##### Set the current working directory\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Copy/SCRIPTS/Forecast_Covid_Recovery/Code/forecast_GDP_v2.ipynb#ch0000004?line=59'>60</a>\u001b[0m path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39me:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCopy\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mSCRIPTS\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mForecast Covid Recovery\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCode\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Copy/SCRIPTS/Forecast_Covid_Recovery/Code/forecast_GDP_v2.ipynb#ch0000004?line=60'>61</a>\u001b[0m os\u001b[39m.\u001b[39;49mchdir(path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Copy/SCRIPTS/Forecast_Covid_Recovery/Code/forecast_GDP_v2.ipynb#ch0000004?line=62'>63</a>\u001b[0m path_midas \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./midas/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Copy/SCRIPTS/Forecast_Covid_Recovery/Code/forecast_GDP_v2.ipynb#ch0000004?line=63'>64</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(path_midas)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'e:\\\\Copy\\\\SCRIPTS\\\\Forecast Covid Recovery\\\\Code\\\\'"
     ]
    }
   ],
   "source": [
    "# ================================ ML with H2O: Calculate in-sample and out-of-sample MAD, MAE, RMSE, R^2 ================================= #\n",
    "# =========================================================================================================================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from varname import nameof\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import robust\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import H2O\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.estimators import H2OXGBoostEstimator # (not supported on Windows OS)\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "# import a Lasso module for time series analysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import Random Forest module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# import XGBoost module\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# import Prophet\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "from fbprophet.utilities import regressor_coefficients \n",
    "import utils_fprophet\n",
    "\n",
    "# import tensorflow\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation, LSTM, Dropout, SimpleRNN\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.backend import clear_session\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import multiprocessing as multi\n",
    "\n",
    "##### Set the current working directory\n",
    "path=\"e:\\\\Copy\\\\SCRIPTS\\\\Forecast_Covid_Recovery\\\\Code\\\\\"\n",
    "os.chdir(path)\n",
    "\n",
    "path_midas = \"./midas/\"\n",
    "sys.path.append(path_midas)\n",
    "\n",
    "from mix import mix_freq, mix_freq2\n",
    "from adl import estimate, forecast, midas_adl, rmse, estimate2,forecast2, midas_adl2\n",
    "\n",
    "\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '30'\n",
    "\n",
    "\n",
    "##### Call H2O algorithms to perform out-of-sample forecast\n",
    "def H2Of(R: np.array, X: np.array, tau: int, use_model = 'randomforest', max_models = 3, max_runtime_secs = 600, max_runtime_secs_per_model = 40) -> tuple:\n",
    "    \"\"\"\n",
    "        R: a numpy vector of responses\n",
    "        X: a numpy array of predictors\n",
    "        tau: a forecast horizon \n",
    "        max_models: a maximum number of models to be trained\n",
    "        max_runtime_secs: a maximum runtime\n",
    "        max_runtime_secs_per_model: a maximum runtime to train each individual model\n",
    "    \"\"\"\n",
    "    assert (R.shape[0] == X.shape[0]), \"numbers of rows not match!\"\n",
    "    assert(tau > 0), \"the forecast horizon must be greater than zero!\"\n",
    "    \n",
    "    T = X.shape[0]\n",
    "    dim = X.shape[1]\n",
    "    # R = R.flatten()\n",
    "    # X = X.flatten() # flatten arrays\n",
    "\n",
    "    R1 = np.empty( shape = (0, 1) )\n",
    "    X1 = np.empty( shape = (0, dim) )\n",
    "    for t in np.arange(0, T):\n",
    "        if t < T-tau:\n",
    "            R1 = np.append(R1, R[t+tau].reshape(1, 1), axis = 0)\n",
    "        else:\n",
    "            R1 = np.append(R1, np.array([0]).reshape(1, 1), axis = 0)\n",
    "        X1 = np.append(X1, X[t, :].reshape(1, dim), axis = 0)\n",
    "\n",
    "    # preprocess data\n",
    "    if use_model == 'deeplearning':\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(X1)\n",
    "\n",
    "    # split the numpy array into train, validation, and test data\n",
    "    sratio = 0.8\n",
    "    X1_train, X1_valid, X1_test = X1[0:(math.floor(sratio*T)-tau), :].reshape(-1, dim), X1[(math.floor(sratio*T)-tau):(T-tau), :].reshape(-1, dim), \\\n",
    "                                                                                                                                                                                X1[(T-tau):, :].reshape(-1, dim)\n",
    "    R1_train, R1_valid, R1_test = R1[0:(math.floor(sratio*T)-tau), :].reshape(-1, 1), R1[(math.floor(sratio*T)-tau):(T-tau), :].reshape(-1, 1), \\\n",
    "                                                                                                                                                                                    R1[(T-tau):, :].reshape(-1, 1)\n",
    "\n",
    "    # put all data into H2O dataframes\n",
    "    train_hf = h2o.H2OFrame(python_obj=np.concatenate( (R1_train, X1_train), axis = 1 ), column_names=['x'+str(i) for i in range(dim+1)])\n",
    "    train_hf.rename(columns = {'x0': 'y'}) \n",
    "    valid_hf = h2o.H2OFrame(python_obj=np.concatenate( (R1_valid, X1_valid), axis = 1 ), column_names=['x'+str(i) for i in range(dim+1)])\n",
    "    valid_hf.rename(columns = {'x0': 'y'}) \n",
    "    test_hf = h2o.H2OFrame(python_obj=np.concatenate( (R1_test, X1_test), axis = 1 ), column_names=['x'+str(i) for i in range(dim+1)])\n",
    "    test_hf.rename(columns = {'x0': 'y'}) \n",
    "\n",
    "    # build and train the model\n",
    "    if use_model == 'randomforest':\n",
    "        model = H2OAutoML(project_name = \"AutoML_RF\", max_models = max_models, max_runtime_secs = max_runtime_secs, nfolds = 0, \\\n",
    "                                                max_runtime_secs_per_model = max_runtime_secs_per_model, include_algos = [\"DRF\"], keep_cross_validation_predictions = True, \\\n",
    "                                                stopping_metric = 'RMSE', sort_metric = 'RMSE', seed = 1234, verbosity = 'warn')\n",
    "    elif use_model == 'gbm':\n",
    "        model = H2OAutoML(project_name = \"AutoML_GBM\", max_models = max_models, max_runtime_secs = max_runtime_secs, nfolds = 0, \\\n",
    "                                                max_runtime_secs_per_model = max_runtime_secs_per_model, include_algos = [\"GBM\"], keep_cross_validation_predictions = True, \\\n",
    "                                                stopping_metric = 'RMSE', sort_metric = 'RMSE', seed = 1234, verbosity = 'warn')\n",
    "    elif use_model == 'xgboost':\n",
    "        model = H2OAutoML(project_name = \"AutoML_XGB\", max_models = max_models, max_runtime_secs = max_runtime_secs, nfolds = 0, \\\n",
    "                                                max_runtime_secs_per_model = max_runtime_secs_per_model, include_algos = [\"XGBoost\"], keep_cross_validation_predictions = True, \\\n",
    "                                                stopping_metric = 'RMSE', sort_metric = 'RMSE', seed = 1234, verbosity = 'warn')\n",
    "    elif use_model == 'deeplearning':\n",
    "        model = H2OAutoML(project_name = \"AutoML_DL\", max_models = max_models, max_runtime_secs = max_runtime_secs, nfolds = 0, \\\n",
    "                                                max_runtime_secs_per_model = max_runtime_secs_per_model, include_algos = [\"DeepLearning\"], keep_cross_validation_predictions = True, \\\n",
    "                                                stopping_metric = 'RMSE', sort_metric = 'RMSE', seed = 1234, verbosity = 'warn')\n",
    "    elif use_model == 'glm':\n",
    "        model = H2OAutoML(project_name = \"AutoML_GLM\", max_models = max_models, max_runtime_secs = max_runtime_secs, nfolds = 0, \\\n",
    "                                                max_runtime_secs_per_model = max_runtime_secs_per_model, include_algos = [\"GLM\"], keep_cross_validation_predictions = True, \\\n",
    "                                                stopping_metric = 'RMSE', sort_metric = 'RMSE', seed = 1234, verbosity = 'warn')\n",
    "    else:\n",
    "        print(f'Model {use_model} does not exist!')\n",
    "        sys.exit()\n",
    "    model.train(x=list(set(train_hf.names)-{\"y\"}), y=\"y\", training_frame=train_hf, validation_frame=valid_hf)\n",
    "\n",
    "    # get optimal hyperparameters\n",
    "    best_model = model.get_best_model(criterion='RMSE')\n",
    "    opt_params = best_model.summary().as_data_frame().iloc[:, 1:]\n",
    "    opt_params = opt_params.to_dict('records')[0]\n",
    " \n",
    "    # get performance measures for the validation data\n",
    "    best_model_perf = best_model.model_performance(valid_hf)\n",
    "    rmse = best_model_perf.rmse()\n",
    "    mae = best_model_perf.mae()\n",
    "\n",
    "    # forecast for the test data\n",
    "    forecasts = model.predict(test_hf).as_data_frame(use_pandas=True, header=True).values\n",
    "    # print('forecasts = \\n', forecasts)\n",
    "    forecast_tau = float(forecasts[len(forecasts) - 1])\n",
    "\n",
    "    # delete all the H2O data and model\n",
    "    h2o.remove(model)\n",
    "    h2o.remove(best_model)\n",
    "    h2o.remove(train_hf)\n",
    "    h2o.remove(valid_hf)\n",
    "    h2o.remove(test_hf)\n",
    "\n",
    "    # output results\n",
    "    return forecast_tau, rmse, mae, opt_params\n",
    "\n",
    "##### Calculate rolling-window OoS R^2 for 'tau'-steps ahead forecasts\n",
    "def OoS_R_sq(df: pd.DataFrame, tau: int, wsize: int, sstart:int, T1: int, ylag: int, use_model = 'randomforest', max_models = 3, \\\n",
    "                                                                                                                                    max_runtime_secs = 600, max_runtime_secs_per_model = 40) -> tuple:\n",
    "    \"\"\" \n",
    "        'df': a pandas dataframe\n",
    "        'tau': forecast horizon\n",
    "        'wsize': window size,\n",
    "        'sstart': sub-sample starting point\n",
    "        'T1': size of a sub-sample\n",
    "        'ylag': AR lag of the dependent variable \n",
    "        'max_models': a maximum number of models to be trained\n",
    "        'max_runtime_secs': a maximum runtime \n",
    "        'max_runtime_secs_per_model': a maximum runtime to train each individual model\n",
    "    \"\"\"\n",
    "    assert (T1 > wsize+tau), \"size of a subsample must be much greater than the window size!\"\n",
    "    assert ( sstart+T1 <= len(df) ), \"end point of the last subsample must be less than or equal to size of the dataframe!\"\n",
    "\n",
    "    df = df.iloc[sstart:(sstart+T1), :].copy()\n",
    "    # get data for the dependant variable and predictors\n",
    "    if ylag > 0:\n",
    "        for i in np.arange(1, ylag+1):\n",
    "            df[f'ylag{i}'] = df.iloc[:, 1].shift(i)\n",
    "        df.dropna(inplace = True)\n",
    "    # print( df.iloc[0, 0] )\n",
    "\n",
    "    R = np.array(df.values[:, 1], dtype='float64')\n",
    "    X = np.array(df.values[:, 2:], dtype='float64')\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    rmse = 0\n",
    "    var = 0\n",
    "    mae = 0\n",
    "    list_err = []\n",
    "    rmse_in_ls, mae_in_ls = [], []\n",
    "    list_opt_params_vlues = []\n",
    "    for s in np.arange(T1-wsize-tau-ylag):\n",
    "        # print('iter: ', s)\n",
    "        # estimate a long-run regression model and make a 'tau'-steps ahead forecast\n",
    "        if use_model == use_model:\n",
    "            r_forecast, rmse_in, mae_in, opt_params = H2Of(R[s:(s+wsize+1)].reshape(-1, 1), X[s:(s+wsize+1), :].reshape(-1, dim), tau, \\\n",
    "                                                                                            use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                                            max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            list_opt_params_keys = [k for k in opt_params.keys()]\n",
    "            list_opt_params_vlues.append( list( opt_params.values() ) )\n",
    "            rmse_in_ls.append(rmse_in)\n",
    "            mae_in_ls.append(mae_in)\n",
    "        else:\n",
    "            print(f'Model {use_model} does not exist!')\n",
    "            sys.exit()\n",
    "\n",
    "        r = R[s+wsize+tau] # actual returns\n",
    "        rmse +=  pow(r - r_forecast, 2) / (T1-wsize-tau-ylag)\n",
    "        var += pow(r - np.mean(R[s:(s+wsize+1)]), 2) / (T1-wsize-tau-ylag)\n",
    "        mae += abs(r - r_forecast) / (T1-wsize-tau-ylag)\n",
    "        list_err.append(r - r_forecast)\n",
    "    err = np.array(list_err)\n",
    "    mad = robust.mad(err, c = 1)\n",
    "\n",
    "    # save optimal hyperparameters to a dataframe\n",
    "    list_opt_params_vlues = np.array(list_opt_params_vlues)\n",
    "    opt_params_df = pd.DataFrame({list_opt_params_keys[i]: list_opt_params_vlues[:,i] for i in np.arange( len(list_opt_params_keys) )})\n",
    "    \n",
    "    del df # delete this copy of the dataframe\n",
    "    return float(mad), float(mae), math.sqrt(float(rmse) ), float(1 - rmse/(var + 0.00001) ), np.median(rmse_in_ls), np.median(mae_in_ls), \\\n",
    "                                                                                                                                                    opt_params_df.select_dtypes(include=np.number).median(axis=0)\n",
    "\n",
    "##### Calculate values of the rolling-window OoS R^2 of 'tau'-steps ahead forecasts for many different moving-forward sub-samples\n",
    "def OoS_R_sq_many_ssamples(df: pd.DataFrame, df_name: 'string', tau: int, step_size, wsize: int, T1, ylag, use_model = 'deeplearning', \\\n",
    "                                                    max_models = 100, max_runtime_secs = 600, max_runtime_secs_per_model = 40) -> tuple:\n",
    "    ssize = len(df) - ylag                                                                                                                                                                                                        \n",
    "    print(f'Entire sample size = {ssize}; Sub-sample size = {T1}; rolling-window size = {wsize}; AR lag = {ylag}; forecast horizon = {tau}; model = {use_model}')\n",
    "    perf_dict = {'sstart': [], 'ssample_start_date': [], 'ssample_end_date': [], 'mad': [], 'mae': [], 'rmse': [], 'r_sq': [], 'rmse_in_med': [], 'mae_in_med': []}\n",
    "    opt_params_med_ls = []\n",
    "    for sstart in np.arange(0, ssize+step_size, step_size):\n",
    "        if sstart+T1 <= ssize:\n",
    "            mad, mae, rmse, r_sq, rmse_in_med, mae_in_med, opt_params_med = OoS_R_sq(df, tau, wsize, sstart, T1, ylag, use_model = use_model, \\\n",
    "                                                                                                                                                    max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                                                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print(f'MAD = {mad:.4f}, MAE = {mae:.4f}, RMSE = {rmse:.4f}, OoS R^2 = {r_sq:.4f}')\n",
    "            perf_dict['sstart'].append(sstart)\n",
    "            perf_dict['ssample_start_date'].append(df.iloc[sstart, 0])\n",
    "            perf_dict['ssample_end_date'].append(df.iloc[sstart+T1-1, 0])\n",
    "            perf_dict['mad'].append(f'{mad:.4f}')\n",
    "            perf_dict['mae'].append(f'{mae:.4f}')\n",
    "            perf_dict['rmse'].append(f'{rmse:.4f}')\n",
    "            perf_dict['r_sq'].append(f'{r_sq:.4f}')\n",
    "            perf_dict['rmse_in_med'].append(f'{rmse_in_med:.4f}')\n",
    "            perf_dict['mae_in_med'].append(f'{mae_in_med:.4f}')\n",
    "            opt_params_med_columns = list(opt_params_med.index)\n",
    "            opt_params_med_ls.append( list(opt_params_med.values) )\n",
    "    # save performance metrics to a data frame\n",
    "    perf_df = pd.DataFrame.from_dict(perf_dict)\n",
    "    perf_df.to_csv(f'../Data/FRED/perf_out_{df_name}_model_{use_model}_ssize_{ssize}_subsize_{T1}_fhorizon_{tau}_ylag_{ylag}.csv', index=True, header = True)\n",
    "\n",
    "    # save medians of optimal hyperparameters to a data frame\n",
    "    opt_params_med_ls = np.array(opt_params_med_ls)\n",
    "    opt_params_med_df = pd.DataFrame({opt_params_med_columns[i]: opt_params_med_ls[:,i] for i in np.arange( len(opt_params_med_columns) )})\n",
    "    opt_params_med_df.insert(loc = 0, column = 'sstart', value = perf_df.sstart)\n",
    "    opt_params_med_df.insert(loc = 1, column = 'ssample_start_date', value = perf_df.ssample_start_date)\n",
    "    opt_params_med_df.insert(loc = 2, column = 'ssample_end_date', value = perf_df.ssample_end_date)\n",
    "    opt_params_med_df.to_csv(f'../Data/FRED/opt_params_out_{df_name}_model_{use_model}_ssize_{ssize}_subsize_{T1}_fhorizon_{tau}_ylag_{ylag}.csv', index=True, header = True)\n",
    "    return perf_df, opt_params_med_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    ##### Import data\n",
    "    # import a sample with many predictors\n",
    "    US_df_big = pd.read_csv('../Data/FRED/quarterly_transformed_pca.csv', engine = 'python', encoding='utf-8', skipinitialspace=True, sep = ',', parse_dates=True)\n",
    "    display(US_df_big.head())\n",
    "\n",
    "    # import a sample with a few predictors\n",
    "    US_df_small = pd.read_csv('../Data/FRED/quarterly_transformed_pdc_sis.csv', engine = 'python', encoding='utf-8', \\\n",
    "                                                                                                                       skipinitialspace=True, sep = ',', parse_dates=True)\n",
    "    \n",
    "\t# import ADSI in addition to FRED variables selected by variable screening \n",
    "    US_df_ADS_small = pd.read_csv('../Data/combined_data_ADSI.csv', engine = 'python', encoding='utf-8', skipinitialspace=True, sep = ',', parse_dates=True)\n",
    "                                                                                                                                                                                    \n",
    "\n",
    "    ##### Launch a H2O instance\n",
    "    h2o.init(ip=\"localhost\", port=54323, max_mem_size_GB=100, nthreads=-1) \n",
    "    # h2o.cluster().shutdown()\n",
    "    h2o.no_progress() # suppress progress bars\n",
    "\n",
    "\n",
    "    batch_size = 40\n",
    "    num_epochs = 100\n",
    "    wsize = 60\n",
    "    ylag = 1\n",
    "    ssize = len(US_df_big) - ylag\n",
    "    step = 1\n",
    "    T1 = 100\n",
    "    max_models = 10\n",
    "    max_runtime_secs = 0\n",
    "    max_runtime_secs_per_model = 20\n",
    "\n",
    "     ####################################################### Forecast with 'US_df_big' #######################################################################\n",
    "    tau_step = 1\n",
    "    taus = np.arange(1, 4+tau_step, tau_step)\n",
    "    for tau in taus:\n",
    "        use_model = 'deeplearning'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_big, 'US_df_big', tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "        use_model = 'randomforest'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_big, nameof(US_df_big), tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "        use_model = 'gbm'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_big, 'US_df_big', tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "        use_model = 'xgboost'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_big, 'US_df_big', tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "    ####################################################### Forecast with 'US_df_small' ###########################################################################\n",
    "\n",
    "    tau_step = 1\n",
    "    taus = np.arange(1, 4+tau_step, tau_step)\n",
    "    for tau in taus:\n",
    "        if tau > 2:\n",
    "            use_model = 'randomforest'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_small, nameof(US_df_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "            use_model = 'gbm'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_small, nameof(US_df_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "            use_model = 'deeplearning'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_small, nameof(US_df_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "            use_model = 'xgboost'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_small, nameof(US_df_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "            use_model = 'glm'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_small, nameof(US_df_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "\n",
    "\n",
    "    ####################################################### Forecast with 'US_df_ADS_small' #######################################################################\n",
    "\n",
    "    tau_step = 1\n",
    "    taus = np.arange(1, 4+tau_step, tau_step)\n",
    "    for tau in taus:\n",
    "        if tau > 1:\n",
    "            use_model = 'randomforest'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_ADS_small, nameof(US_df_ADS_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "            use_model = 'gbm'\n",
    "            perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_ADS_small, nameof(US_df_ADS_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                    use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                    max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "            print( perf_df.head() )\n",
    "            print( opt_params_med_df.head() )\n",
    "\n",
    "        use_model = 'deeplearning'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_ADS_small, nameof(US_df_ADS_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "        use_model = 'xgboost'\n",
    "        perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_ADS_small, nameof(US_df_ADS_small), tau, step, wsize, T1, ylag, \\\n",
    "                                                                use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "                                                                max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        print( perf_df.head() )\n",
    "        print( opt_params_med_df.head() )\n",
    "\n",
    "        # use_model = 'glm'\n",
    "        # perf_df, opt_params_med_df = OoS_R_sq_many_ssamples(US_df_ADS_small, nameof(US_df_ADS_small), tau, step, wsize, T1, ylag, \\\n",
    "        #                                                         use_model = use_model, max_models = max_models, max_runtime_secs = max_runtime_secs, \\\n",
    "        #                                                         max_runtime_secs_per_model = max_runtime_secs_per_model)\n",
    "        # print( perf_df.head() )\n",
    "        # print( opt_params_med_df.head() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### Close the H2O instance\n",
    "    h2o.cluster().shutdown()\n",
    "\n",
    "    print( 'Completed in: %s sec'%(time.time() - start_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc917ebd80857c8984278d262088eb38f6d12477c79ef5a6d94d7dc2b555f094"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
